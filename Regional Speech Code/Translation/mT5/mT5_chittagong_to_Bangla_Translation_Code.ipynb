{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8qkhqIo0FMM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxoNB55w0Rov",
    "outputId": "b14540b2-dea0-4c23-a70d-bd5c4e517435"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N26NczH_0Rru"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"/content/Chittagong_Train - Sheet1.csv\")\n",
    "test_data = pd.read_csv(\"/content/chittagong_test - Sheet1.csv\")\n",
    "validation_data = pd.read_csv(\"/content/chittagong_validation - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "lPw8DnT11xk1",
    "outputId": "c2e38dd9-8a50-4e25-b8cd-d2685a8a3773"
   },
   "outputs": [],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "ZeMgko9S2W-H",
    "outputId": "859362f9-886a-461f-b981-605883c0981c"
   },
   "outputs": [],
   "source": [
    "# Rename the columns to match the expected format\n",
    "train_data.rename(columns={'chittagong_bangla_speech': 'input_text', 'bangla_speech': 'labels'}, inplace=True)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HUAW0qqt1x_V",
    "outputId": "0cd6ca5b-e67f-45e0-bdf9-c5d006edf6a8"
   },
   "outputs": [],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ICeu3MvG2Xkn",
    "outputId": "7884ad7f-cf82-422f-933f-f9bb6d6f5b2c"
   },
   "outputs": [],
   "source": [
    "# Rename the columns to match the expected format\n",
    "test_data.rename(columns={'chittagong_bangla_speech': 'input_text', 'bangla_speech'\t: 'labels'}, inplace=True)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "EbjGz7KX11vG",
    "outputId": "53c92658-5a16-4141-f4c6-83f6f6fa1807"
   },
   "outputs": [],
   "source": [
    "validation_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "vxr9N4Oz2ZUL",
    "outputId": "4bdebda3-af73-4ca0-bf6f-d86e299ca13b"
   },
   "outputs": [],
   "source": [
    "# Rename the columns to match the expected format\n",
    "validation_data.rename(columns={'chittagong_bangla_speech': 'input_text', 'bangla_speech'\t: 'labels'}, inplace=True)\n",
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvZU0YCz5S6T",
    "outputId": "59deae03-c34b-4e0b-eef6-608a1832df8e"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewgAuAdN5lOD",
    "outputId": "263db2cd-55fa-4954-e7e6-ca9e10ffabc4"
   },
   "outputs": [],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSWbj-Qx5lRB",
    "outputId": "996ab465-cabd-4097-c24e-a951428fef58"
   },
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gksX0-7G5lWi",
    "outputId": "c1dff0e3-f955-4ccc-94c5-1a7a4b2fa2d8"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nikoCtvm5lZ7",
    "outputId": "f16be831-20d9-4cf8-83a4-3b30f024a668"
   },
   "outputs": [],
   "source": [
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7tBHm2v5th9",
    "outputId": "8debd97d-6db2-459b-ab8a-9c4c9b12ad30"
   },
   "outputs": [],
   "source": [
    "!transformers-cli cache clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ir8YLuX5t3k",
    "outputId": "a236cd24-1f75-42a8-bd22-148cafe633d5"
   },
   "outputs": [],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-2iB14y6DJv",
    "outputId": "bf5c4410-1647-497c-b2fb-3fad4d0c7149"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_Be4m5g6Ewv",
    "outputId": "fd1120ff-4fc8-4bc6-e31d-e19644b913ff"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/csebuetnlp/normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ft7z0inq6GR4",
    "outputId": "3361e962-e3f1-4063-e15e-05060efd5595"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WqqErcR6GoA",
    "outputId": "33fa0c85-6174-495a-c295-15b4dd0d2e76"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysKhQyvN6I_o",
    "outputId": "f8e046d7-0335-400a-f265-a6604ba63d2a"
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4_9m3MP6K_R",
    "outputId": "17d118a3-9167-4be5-c208-131ae8e8adb2"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate==0.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IyFWplMAtNq_",
    "outputId": "432a2806-0b74-4fdf-a896-0eebd327f920"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/csebuetnlp/normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JtTux4mnn6Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7wCqcXJnPB-"
   },
   "source": [
    "# **Load the model and tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0ehjo-qnous",
    "outputId": "b6331124-8400-44b1-f6e4-c906ef6374cb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "#https://huggingface.co/docs/transformers/model_doc/mt5\n",
    "model_name = \"google/mt5-small\" # The variations it has -> mt5-small: 6, mt5-base: 12,mt5-large: 24, mt5-xl: 24, mt5-xxl: 24\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY2lps5K49Ei"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from normalizer import normalize\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer ,DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "# Load the saved model\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/movie/mymensingh_translation_mT5.pt\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/movie/mymensingh_tokenizer_mT5.json\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fxLl_2k27zr"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: A DataFrame containing 'input_text' and 'labels' columns.\n",
    "            tokenizer: A Hugging Face tokenizer.\n",
    "            max_length: Maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.input_text = data['input_text'].apply(normalize).tolist()\n",
    "        self.labels = data['labels'].apply(normalize).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.input_text[idx]\n",
    "        label_text = self.labels[idx]\n",
    "\n",
    "        # Tokenize the input text\n",
    "        input_encodings = self.tokenizer(\n",
    "            input_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Tokenize the label text to get its 'input_ids' and 'attention_mask'\n",
    "        label_encodings = self.tokenizer(\n",
    "            label_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': input_encodings['attention_mask'].squeeze(),\n",
    "            'labels': label_encodings['input_ids'].squeeze(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpdutUxB2727"
   },
   "outputs": [],
   "source": [
    "# Create train , test and validation datasets\n",
    "train_dataset = Seq2SeqDataset(train_data, tokenizer)\n",
    "test_dataset = Seq2SeqDataset(test_data, tokenizer)\n",
    "validation_dataset = Seq2SeqDataset(validation_data, tokenizer)\n",
    "\n",
    "# Create train , test and validation dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)  #batch_size=32\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16) #batch_size=32\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16) #batch_size=32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVyxO66e3F9Q",
    "outputId": "8155e05f-ebdc-41b5-a0a7-dfdd8d5e1397"
   },
   "outputs": [],
   "source": [
    "# Move the model to the device (CPU or GPU)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIYgPkGm9jli"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AdamW\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Create a custom optimizer using torch.optim.AdamW\n",
    "custom_optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,  # Learning rate\n",
    "    eps=1e-8,  # Epsilon value to prevent division by zero\n",
    "    weight_decay=0.01,  # Weight decay (L2 regularization)\n",
    ")\n",
    "\n",
    "# Define the TrainingArguments for fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/drive/MyDrive/movie/mymensingh_translation_mT5/model_fine_tuned',\n",
    "    num_train_epochs=20,  # You can adjust the number of epochs\n",
    "    per_device_train_batch_size=6,  # You can adjust the batch size\n",
    "    gradient_accumulation_steps=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    "    save_steps=15000,\n",
    "    learning_rate=1e-3,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/content/drive/MyDrive/movie/mymensingh_translation_mT5/model_fine_tuned',\n",
    "    logging_steps=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtJSY_gF3Zs5"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Create a data collator for sequence-to-sequence tasks\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,  # Your Hugging Face tokenizer\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    max_length=128,\n",
    "    label_pad_token_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHBr8I2f1Ekc"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,  # Use the model you loaded\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  # Use your data collator\n",
    "    train_dataset=train_dataset,  # Use your training dataset\n",
    "    eval_dataset=validation_dataset,  # Use your evaluation dataset\n",
    "    optimizers=(custom_optimizer, None),  # Use your custom optimizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "AzroKJHm-50G",
    "outputId": "50dd9584-6df5-4088-bf53-79e71275a5b8"
   },
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qQMPeD3lifb"
   },
   "source": [
    "# **Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIsLiye31J-1",
    "outputId": "a3a133b3-1243-4672-990e-ba32cf356e44"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"/content/drive/MyDrive/movie/mymensingh_translation_mT5.pt\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/movie/mymensingh_tokenizer_mT5.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtQNx_UWllTr"
   },
   "source": [
    "# **Load the model again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhJTzFGf31jO"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the saved model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/drive/MyDrive/movie/mymensingh_translation_mT5.pt\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/movie/mymensingh_tokenizer_mT5.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4931GPs35p2",
    "outputId": "8be878a2-edd7-4832-b432-02ff320a20bf"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0LySOpK-4Ej6",
    "outputId": "9d596cff-542c-4aac-8ca6-d3cd2841871e"
   },
   "outputs": [],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-cSDMgC4Eoh",
    "outputId": "ed1a5dcf-0644-4265-d1f4-4bcb71aa3a8b"
   },
   "outputs": [],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJS-YMDM4Erp",
    "outputId": "2ec33894-8b5f-4b3c-9394-dc9b58ea7226"
   },
   "outputs": [],
   "source": [
    "# Move the model to the device (CPU or GPU)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtdovWlt4JKz",
    "outputId": "2d70d337-6f15-4872-e308-54081380c495"
   },
   "outputs": [],
   "source": [
    "!pip install rouge-score\n",
    "#https://github.com/google-research/google-research/tree/master/rouge\n",
    "#https://huggingface.co/spaces/evaluate-metric/rouge [Different types of ROUGE scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwOqAGUB4JNr",
    "outputId": "cb871d2b-88a0-47e7-bcc7-cb32d6d652d9"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEpSQZQi4JRE",
    "outputId": "27fca048-0d99-44f8-966f-944c7012313a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import Levenshtein\n",
    "from evaluate import load\n",
    "# Define the move_to_device function\n",
    "def move_to_device(batch, device):\n",
    "    if isinstance(batch, torch.Tensor):\n",
    "        return batch.to(device)\n",
    "    elif isinstance(batch, list):\n",
    "        return [move_to_device(item, device) for item in batch]\n",
    "    elif isinstance(batch, dict):\n",
    "        return {key: move_to_device(value, device) for key, value in batch.items()}\n",
    "    else:\n",
    "        return batch  # If it's not a tensor, list, or dict, leave it as is\n",
    "\n",
    "# Load the evaluation metric for Character Error Rate (CER) and Word Error Rate (WER) and Exact Match(em)\n",
    "cer_metric = load(\"cer\")\n",
    "wer_metric = load(\"wer\")\n",
    "meteor = load('meteor')\n",
    "exact_match_metric = load(\"exact_match\")\n",
    "\n",
    "# Load BLEU and ROUGE metrics\n",
    "bleu_metric = load(\"bleu\")\n",
    "rouge_metric = load('rouge')\n",
    "\n",
    "# Initialize lists to store generated translations and references\n",
    "generated_translations = []\n",
    "references = []\n",
    "\n",
    "# Generate translations for the test dataset\n",
    "for batch in test_dataloader:\n",
    "    # Move the batch to CUDA\n",
    "    batch = move_to_device(batch, 'cuda')\n",
    "\n",
    "    input_text = batch['input_ids']  # Access the input_text using the correct key\n",
    "    labels = batch['labels']  # Access the labels using the correct key\n",
    "\n",
    "    # Generate translations\n",
    "    translation_ids = model.generate(input_text, max_length=128, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
    "\n",
    "    # Move the translation_ids to CPU to decode\n",
    "    translation_ids = translation_ids.to('cpu')\n",
    "\n",
    "    generated_translation = tokenizer.batch_decode(translation_ids, skip_special_tokens=True)\n",
    "\n",
    "    generated_translations.extend(generated_translation)\n",
    "    references.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))  # Decoding the label IDs\n",
    "\n",
    "# Make sure to move generated_translations back to CPU for evaluation if necessary\n",
    "generated_translations = [translation if not isinstance(translation, str) else translation for translation in generated_translations]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cg69Lnfm4PyI",
    "outputId": "c9e418d5-aaea-43f1-dd65-01660f33ed75"
   },
   "outputs": [],
   "source": [
    "print(\"Number of generated translations:\", len(generated_translations))\n",
    "print(\"Number of references:\", len(references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_AS7K9P4P1G",
    "outputId": "a6934c52-925b-470a-d7c4-d83a93785d2e"
   },
   "outputs": [],
   "source": [
    "print(generated_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8Y7DH9r4P3-",
    "outputId": "8199753a-9f88-4483-b897-3beab3377ecf"
   },
   "outputs": [],
   "source": [
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TLPIr5Z4Uc4"
   },
   "outputs": [],
   "source": [
    "# Calculate Character Error Rate (CER) and Word Error Rate (WER)\n",
    "results_CER = cer_metric.compute(predictions=generated_translations, references=references)\n",
    "results_WER = wer_metric.compute(predictions=generated_translations, references=references)\n",
    "\n",
    "# Calculate Exact Match (EM) and METEOR(M)\n",
    "results_em = exact_match_metric.compute(predictions=generated_translations, references=references)\n",
    "results_met = meteor.compute(predictions=generated_translations, references=references)\n",
    "\n",
    "# Calculate Bilingual Evaluation Understudy (BLEU) and Recall-Oriented Understudy for Gisting Evaluation (ROUGE)\n",
    "results_bleu = bleu_metric.compute(predictions=generated_translations, references=references)\n",
    "results_rouge = rouge_metric.compute(predictions=generated_translations, references=references)\n",
    "\n",
    "\n",
    "# Calculate Levenshtein Distance\n",
    "levenshtein_distances = [Levenshtein.distance(generated, reference) for generated, reference in zip(generated_translations, references)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1j43gyg4Ufx",
    "outputId": "479b3806-2beb-4aa5-dac8-f9e024cd10e5"
   },
   "outputs": [],
   "source": [
    "print(results_CER)\n",
    "print(results_WER)\n",
    "print(results_em)\n",
    "print(results_met)\n",
    "print(results_bleu)\n",
    "print(results_rouge)\n",
    "print(levenshtein_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAW7J9vB4Uio",
    "outputId": "36e57d51-8541-419d-c2b3-10735b2410cf"
   },
   "outputs": [],
   "source": [
    "total_correct = 0\n",
    "total_samples = len(references)\n",
    "\n",
    "for generated, reference in zip(generated_translations, references):\n",
    "    levenshtein_distance = Levenshtein.distance(generated, reference)\n",
    "    max_length = max(len(generated), len(reference))\n",
    "    accuracy = 1 - (levenshtein_distance / max_length)\n",
    "    if accuracy >= 0.7:  # Adjust the threshold as needed\n",
    "        total_correct += 1\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_Ltrb2plro-"
   },
   "source": [
    "# **Save translation results to a csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6lkFghx4Yb0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store translations\n",
    "translation_df = pd.DataFrame({\n",
    "    'input_text': test_data['input_text'],  # Assuming 'test_data' contains your test dataset\n",
    "    'labels': references,\n",
    "    'translations': generated_translations\n",
    "})\n",
    "\n",
    "# Save translations to a CSV file\n",
    "#translation_df.to_csv(\"/content/drive/MyDrive/sylhet_translation_results/sylhet_translation_mT5/model_fine_tuned/mBERT_sylhet_translation.csv\", index=False)\n",
    "# Save translations to a CSV file\n",
    "translation_df.to_excel('/content/sample_data/chittagong_translations_mT5.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wfJgttirFfT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
